{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import mne\n",
    "import mne_bids\n",
    "from mne.minimum_norm import apply_inverse,apply_inverse_epochs\n",
    "\n",
    "subject_id=\"CB040\"\n",
    "visit_id=\"1\"\n",
    "bids_root=\"/headnode1/abry4213/data/Cogitate_MEG_challenge\"\n",
    "region_option=\"hypothesis_driven\"\n",
    "n_jobs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration='1000ms'\n",
    "subject_time_series_path=f\"{bids_root}/derivatives/MEG_time_series/sub-{subject_id}/ses-{visit_id}/meg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_files = [f\"{subject_time_series_path}/{file}\" for file in os.listdir(subject_time_series_path) if \"1000ms_epoch\" in file]\n",
    "all_time_series_data = pd.concat([pd.read_csv(file) for file in time_series_files])\n",
    "\n",
    "# Average across epochs\n",
    "averaged_by_condition = (all_time_series_data\n",
    "                         .groupby([\"stimulus_type\", \"relevance_type\", \"duration\", \"times\", \"meta_ROI\"], as_index=False)[\"data\"]\n",
    "                         .mean()\n",
    "                         .assign(meta_ROI = lambda x: x.meta_ROI.str.replace(\"_meta_ROI\", \"\"))\n",
    "                         .pivot(index=[\"stimulus_type\", \"relevance_type\", \"duration\", \"times\"], columns=\"meta_ROI\", values=\"data\"))\n",
    "\n",
    "averaged_by_condition.columns = averaged_by_condition.columns.get_level_values(0)\n",
    "averaged_by_condition.reset_index(inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = f\"{bids_root}/derivatives/MEG_time_series/sub-{subject_id}_ses-{visit_id}_meg_{duration}_all_time_series.csv\"\n",
    "averaged_by_condition.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>meta_ROI</th>\n",
       "      <th>stimulus_type</th>\n",
       "      <th>relevance_type</th>\n",
       "      <th>duration</th>\n",
       "      <th>times</th>\n",
       "      <th>Category_Selective</th>\n",
       "      <th>GNWT</th>\n",
       "      <th>IIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1000ms</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.032584</td>\n",
       "      <td>0.022722</td>\n",
       "      <td>-0.004934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1000ms</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>0.040036</td>\n",
       "      <td>0.027676</td>\n",
       "      <td>-0.004166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1000ms</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.054817</td>\n",
       "      <td>0.033102</td>\n",
       "      <td>0.001756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1000ms</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.018389</td>\n",
       "      <td>0.025569</td>\n",
       "      <td>0.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1000ms</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.002571</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>-0.006169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "meta_ROI stimulus_type relevance_type duration  times  Category_Selective  \\\n",
       "0                False     Irrelevant   1000ms -0.500            0.032584   \n",
       "1                False     Irrelevant   1000ms -0.499            0.040036   \n",
       "2                False     Irrelevant   1000ms -0.498            0.054817   \n",
       "3                False     Irrelevant   1000ms -0.497            0.018389   \n",
       "4                False     Irrelevant   1000ms -0.496           -0.002571   \n",
       "\n",
       "meta_ROI      GNWT       IIT  \n",
       "0         0.022722 -0.004934  \n",
       "1         0.027676 -0.004166  \n",
       "2         0.033102  0.001756  \n",
       "3         0.025569  0.003065  \n",
       "4         0.015190 -0.006169  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create a dictionary of ROI labels depending on the type of region subset requested\n",
    "def compute_ROI_labels(labels_atlas, region_option, rois_deriv_root):\n",
    "    # Create dictionary to store labels and vertices\n",
    "    labels_dict = {}\n",
    "    if region_option == 'hypothesis_driven':\n",
    "        # Read GNW and IIT ROI list\n",
    "        f = open(op.join(rois_deriv_root,\n",
    "                        'hypothesis_driven_ROIs.json'))\n",
    "        hypothesis_driven_ROIs = json.load(f)\n",
    "\n",
    "        # GNWT ROIs\n",
    "        print(\"GNWT ROIs:\")\n",
    "        for lab in hypothesis_driven_ROIs['GNWT_ROIs']:\n",
    "            print(lab)\n",
    "            labels_dict[\"GNWT_\"+lab] = np.sum([l for l in labels_atlas if lab in l.name])\n",
    "\n",
    "        # IIT ROIs\n",
    "        print(\"IIT ROIs\")\n",
    "        for lab in hypothesis_driven_ROIs['IIT_ROIs']:\n",
    "            print(lab)\n",
    "            labels_dict[\"IIT_\"+lab] = np.sum([l for l in labels_atlas if lab in l.name])\n",
    "\n",
    "        # Category-selective ROIs\n",
    "        print(\"Category-selective ROIs:\")\n",
    "        for lab in hypothesis_driven_ROIs['Category_Selective_ROIs']:\n",
    "            print(lab)\n",
    "            labels_dict[\"Category_Selective_\"+lab] = np.sum([l for l in labels_atlas if lab in l.name])\n",
    "\n",
    "        # Merge all labels in a single one separatelly for GNW and IIT \n",
    "        labels_dict['GNWT_meta_ROI'] = np.sum([l for l_name, l in labels_dict.items() if 'GNWT' in l_name])\n",
    "        labels_dict['IIT_meta_ROI'] = np.sum([l for l_name, l in labels_dict.items() if 'IIT' in l_name])\n",
    "        labels_dict['Category_Selective_meta_ROI'] = np.sum([l for l_name, l in labels_dict.items() if 'Category_Selective' in l_name])\n",
    "\n",
    "        # Only keep the meta-ROIs\n",
    "        labels_dict = {k: v for k, v in labels_dict.items() if 'meta_ROI' in k}\n",
    "\n",
    "    else:\n",
    "        for label in labels_atlas: \n",
    "            label_name = label.name\n",
    "            labels_dict[label_name] =  np.sum([label])\n",
    "\n",
    "    return labels_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set task\n",
    "bids_task = 'dur'\n",
    "\n",
    "# Read epoched data\n",
    "bids_path_epo = mne_bids.BIDSPath(\n",
    "    root=prep_deriv_root, \n",
    "    subject=subject_id,  \n",
    "    datatype='meg',  \n",
    "    task=bids_task,\n",
    "    session=visit_id, \n",
    "    suffix='epo',\n",
    "    extension='.fif',\n",
    "    check=False)\n",
    "\n",
    "# Use Desikan--Killiany atlas to compute dictionary of labels\n",
    "labels_atlas = mne.read_labels_from_annot(\n",
    "    \"sub-\"+subject_id, \n",
    "    parc='aparc.a2009s',\n",
    "    subjects_dir=fs_deriv_root)\n",
    "labels_dict = compute_ROI_labels(labels_atlas, region_option, rois_deriv_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set task\n",
    "bids_task = 'dur'\n",
    "\n",
    "# Read epoched data\n",
    "bids_path_epo = mne_bids.BIDSPath(\n",
    "    root=prep_deriv_root, \n",
    "    subject=subject_id,  \n",
    "    datatype='meg',  \n",
    "    task=bids_task,\n",
    "    session=visit_id, \n",
    "    suffix='epo',\n",
    "    extension='.fif',\n",
    "    check=False)\n",
    "\n",
    "bids_path_epo_rs = mne_bids.BIDSPath(\n",
    "    root=prep_deriv_root, \n",
    "    subject=subject_id,  \n",
    "    datatype='meg',  \n",
    "    task=bids_task,\n",
    "    session=visit_id, \n",
    "    suffix='epo_rs',\n",
    "    extension='.fif',\n",
    "    check=False)\n",
    "\n",
    "print(\"Loading epochs data\")\n",
    "epochs_all = mne.read_epochs(bids_path_epo.fpath, preload=True)\n",
    "\n",
    "epochs_final = epochs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline correction\n",
    "print(\"Running baseline correction\")\n",
    "b_tmin = -0.5\n",
    "b_tmax = 0.\n",
    "baseline = (b_tmin, b_tmax)\n",
    "epochs_final.apply_baseline(baseline=baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rank + covariance matrices\n",
    "with open(f\"{fwd_deriv_root}/sub-{subject_id}_ses-{visit_id}_task-{bids_task}_rank.pkl\", 'rb') as f:\n",
    "            rank = pickle.load(f)\n",
    "\n",
    "with open(f\"{fwd_deriv_root}/sub-{subject_id}_ses-{visit_id}_task-{bids_task}_common_cov.pkl\", 'rb') as f:\n",
    "            common_cov = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read forward model\n",
    "print(\"Reading forward model\")\n",
    "bids_path_fwd = bids_path_epo.copy().update(\n",
    "        root=fwd_deriv_root,\n",
    "        task=bids_task,\n",
    "        suffix=\"surface_fwd\",\n",
    "        extension='.fif',\n",
    "        check=False)\n",
    "fwd = mne.read_forward_solution(bids_path_fwd.fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inverse operator\n",
    "inverse_operator = mne.minimum_norm.make_inverse_operator(\n",
    "    epochs_final.info,\n",
    "    fwd, \n",
    "    common_cov,\n",
    "    loose=.2,\n",
    "    depth=.8,\n",
    "    fixed=False,\n",
    "    rank=rank,\n",
    "    use_cps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_combs = list(itertools.product(conditions[0],\n",
    "                                    conditions[1],\n",
    "                                    conditions[2]))\n",
    "\n",
    "\n",
    "cond_comb = cond_combs[0]\n",
    "\n",
    "epochs = epochs_final['%s == \"%s\" and %s == \"%s\" and %s == \"%s\"' % (\n",
    "            factor[0], cond_comb[0], \n",
    "            factor[1], cond_comb[1], \n",
    "            factor[2], cond_comb[2])]\n",
    "fname_base = f\"{cond_comb[0]}_{cond_comb[1]}_{cond_comb[2]}\".replace(\" \",\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take just the first epoch\n",
    "epoch_first = epochs_final[0]\n",
    "\n",
    "# Compute inverse solution for each epoch\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr ** 2\n",
    "stcs = apply_inverse_epochs(epoch_first, inverse_operator,\n",
    "                            lambda2=1.0 / snr ** 2, verbose=False,\n",
    "                            method=\"dSPM\", pick_ori=\"normal\")\n",
    "\n",
    "# extract time course in label with pca_flip mode\n",
    "src = inverse_operator['src']\n",
    "\n",
    "# Extract time course for each stc\n",
    "epoch_times_df_list = []\n",
    "for i in range(len(stcs)):\n",
    "\n",
    "    # Find epoch number\n",
    "    epoch_number = i+1\n",
    "\n",
    "    # Find stc\n",
    "    stc = stcs[i]\n",
    "\n",
    "    # Loop over labels        \n",
    "    for label_name, label in labels_dict.items():\n",
    "\n",
    "        # Select data in label\n",
    "        stc_in = stc.in_label(label)\n",
    "\n",
    "        # Extract time course data, averaged across channels within ROI\n",
    "        times = stc_in.times\n",
    "        data = stc_in.data.mean(axis=0)\n",
    "\n",
    "        # Concatenate into dataframe\n",
    "        epoch_df = pd.DataFrame({\n",
    "            'epoch_number': epoch_number,\n",
    "            'stimulus_type': cond_comb[0], \n",
    "            'relevance_type': cond_comb[1],\n",
    "            'duration': cond_comb[2],\n",
    "            'times': times,\n",
    "            'meta_ROI': label_name,\n",
    "            'data': data})\n",
    "        \n",
    "        # Write this epoch to a CSV file\n",
    "        output_CSV_file = op.join(subject_time_series_output_path, f\"{fname_base}_epoch{epoch_number}_{label_name}.csv\")\n",
    "        epoch_df.to_csv(output_CSV_file, index=False)\n",
    "\n",
    "# Concatenate all the epoch times\n",
    "# epoch_times_df = pd.concat(epoch_times_df_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspi_new",
   "language": "python",
   "name": "pyspi_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
